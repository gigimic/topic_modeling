{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Saudis', 'are', 'preparing', 'a', 'report', 'that', 'will', 'acknowledge', 'that'], ['Saudi', 'journalist', 'Jamal', \"Khashoggi's\", 'death', 'was', 'the', 'result', 'of', 'an'], ['interrogation', 'that', 'went', 'wrong,', 'one', 'that', 'was', 'intended', 'to', 'lead'], ['to', 'his', 'abduction', 'from', 'Turkey,', 'according', 'to', 'two', 'sources.']]\n",
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "print(texts)\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/gensim-tutorial/\n",
    "Here texts is the list of list of words from the documents.\n",
    "dictionary is the list of unique tokens (words) or the bag of words\n",
    "dictionary.token2id provides an id to each token(word) \n",
    "dictionary is a dictionary object\n",
    "it is also possible to update an existing dictionary to include the new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
     ]
    }
   ],
   "source": [
    "# Show the word to id map\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(48 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "documents_2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "\n",
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'being': 0, 'clear': 1, 'confidence': 2, 'correct': 3, 'either': 4, 'headed': 5, 'hypothesis': 6, 'is': 7, 'of': 8, 'or': 9, 'prediction': 10, 'state': 11, 'that': 12, 'action': 13, 'best': 14, 'chosen': 15, 'comes': 16, 'course': 17, 'effective': 18, 'fidere': 19, 'from': 20, 'latin': 21, 'most': 22, 'the': 23, 'which': 24, 'word': 25, 'having': 26, 'in': 27, 'means': 28, 'one': 29, 'self': 30, 'therefore': 31, 'to': 32, 'trust': 33, 'arrogance': 34, 'believing': 35, 'comparison': 36, 'hubris': 37, 'something': 38, 'this': 39, 'unmerited': 40, 'are': 41, 'capable': 42, 'excessive': 43, 'not': 44, 'overconfidence': 45, 'someone': 46, 'they': 47, 'when': 48, 'any': 49, 'be': 50, 'belief': 51, 'can': 52, 'failure': 53, 'for': 54, 'regard': 55, 'succeeding': 56, 'without': 57, 'and': 58, 'as': 59, 'because': 60, 'fail': 61, 'fulfilling': 62, 'it': 63, 'lack': 64, 'may': 65, 'prophecy': 66, 'those': 67, 'try': 68, 'ability': 69, 'an': 70, 'have': 71, 'innate': 72, 'rather': 73, 'succeed': 74, 'than': 75, 'with': 76}\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "# Create gensim dictionary form a single text file\n",
    "dictionary_file = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('sample.txt', encoding='utf-8'))\n",
    "\n",
    "# Token to Id map\n",
    "print(dictionary_file.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]\n"
     ]
    }
   ],
   "source": [
    "# create bag of words corpus in gensim\n",
    "# List with 2 sentences\n",
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "\n",
    "# Tokenize the docs\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
    "\n",
    "# Create the Corpus\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "pprint(mycorpus)\n",
    "#> [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple_preprocess utility of gensim creates the list of words from the documnet\n",
    "corpora.Dictionary can be used then to create the dictionary (ie. words with index)\n",
    "mycorpus (list of lists) gives the token index and the number of times it appears in each string.\n",
    "word_counts gives each word and the number of times it appears an a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('dogs', 1), ('let', 1), ('out', 1), ('the', 1), ('who', 1)], [('who', 4)]]\n"
     ]
    }
   ],
   "source": [
    "word_counts = [[(mydict[id], count) for id, count in line] for line in mycorpus]\n",
    "pprint(word_counts)\n",
    "#> [[('dogs', 1), ('let', 1), ('out', 1), ('the', 1), ('who', 1)], [('who', 4)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following is the way if you want to create bag of words from  a very big file, reading line by line instead of loading it altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import nltk\n",
    "nltk.download('stopwords')  # run once\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "class BoWCorpus(object):\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.filepath = path\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        global mydict  # OPTIONAL, only if updating the source dictionary.\n",
    "        for line in smart_open(self.filepath, encoding='latin'):\n",
    "            # tokenize\n",
    "            tokenized_list = simple_preprocess(line, deacc=True)\n",
    "\n",
    "            # create bag of words\n",
    "            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)\n",
    "            \n",
    "            # update the source dictionary (OPTIONAL)\n",
    "            mydict.merge_with(self.dictionary)\n",
    "\n",
    "            # lazy return the BoW\n",
    "            yield bow\n",
    "\n",
    "\n",
    "# Create the Dictionary\n",
    "mydict = corpora.Dictionary()\n",
    "\n",
    "# Create the Corpus\n",
    "bow_corpus = BoWCorpus('sample.txt', dictionary=mydict)  # memory friendly\n",
    "\n",
    "# Print the token_id and count for each line.\n",
    "for line in bow_corpus:\n",
    "    print(line)\n",
    "\n",
    "#> [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n",
    "#> [(12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1)]\n",
    "#> ... truncated ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save a gensim dictionary and corpus to disk and load them back\n",
    "# Save the Dict and Corpus\n",
    "mydict.save('mydict.dict')  # save dict to disk\n",
    "corpora.MmCorpus.serialize('bow_corpus.mm', bow_corpus)  # save corpus to disk\n",
    "\n",
    "# Load them back\n",
    "loaded_dict = corpora.Dictionary.load('mydict.dict')\n",
    "\n",
    "corpus = corpora.MmCorpus('bow_corpus.mm')\n",
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the TFIDF matrix (corpus) in gensimfrom gensim import models\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "documents = [\"This is the first line\",\n",
    "             \"This is the second sentence\",\n",
    "             \"This third document\"]\n",
    "\n",
    "# Create the Dictionary and Corpus\n",
    "mydict = corpora.Dictionary([simple_preprocess(line) for line in documents])\n",
    "corpus = [mydict.doc2bow(simple_preprocess(line)) for line in documents]\n",
    "\n",
    "# Show the Word Weights in Corpus\n",
    "for doc in corpus:\n",
    "    print([[mydict[id], freq] for id, freq in doc])\n",
    "\n",
    "# [['first', 1], ['is', 1], ['line', 1], ['the', 1], ['this', 1]]\n",
    "# [['is', 1], ['the', 1], ['this', 1], ['second', 1], ['sentence', 1]]\n",
    "# [['this', 1], ['document', 1], ['third', 1]]\n",
    "\n",
    "# Create the TF-IDF model\n",
    "tfidf = models.TfidfModel(corpus, smartirs='ntc')\n",
    "\n",
    "# Show the TF-IDF weights\n",
    "for doc in tfidf[corpus]:\n",
    "    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "# [['first', 0.66], ['is', 0.24], ['line', 0.66], ['the', 0.24]]\n",
    "# [['is', 0.24], ['the', 0.24], ['second', 0.66], ['sentence', 0.66]]\n",
    "# [['document', 0.71], ['third', 0.71]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'French', 'Revolution', 'was', 'a', 'watershed', 'event', 'in', 'modern', 'European', 'history', 'that', 'began', 'in', '1789', 'and', 'ended', 'in', 'the', 'late', '1790s', 'with', 'the', 'ascent', 'of', 'Napoleon', 'Bonaparte.', 'Although', 'it', 'failed', 'to', 'achieve', 'all', 'of', 'its', 'goals', 'and', 'at', 'times', 'degenerated', 'into', 'a', 'chaotic', 'bloodbath,', 'the', 'French', 'Revolution', 'played', 'a', 'critical', 'role', 'in', 'shaping', 'modern', 'nations', 'by', 'showing', 'the', 'world', 'the', 'power', 'inherent', 'in', 'the', 'will', 'of', 'the', 'people.']\n",
      "['The', 'French', 'Revolution', 'was', 'a', 'watershed', 'event', 'in', 'modern', 'European', 'history', 'that', 'began', 'in', '1789', 'and', 'ended', 'in', 'the', 'late', '1790s', 'with', 'the', 'ascent', 'of', 'Napoleon', 'Bonaparte.', 'Although', 'it', 'failed', 'to', 'achieve', 'all', 'of', 'its', 'goals', 'and', 'at', 'times', 'degenerated', 'into', 'a', 'chaotic', 'bloodbath,', 'the', 'French', 'Revolution', 'played', 'a', 'critical', 'role', 'in', 'shaping', 'modern', 'nations', 'by', 'showing', 'the', 'world', 'the', 'power', 'inherent', 'in', 'the', 'will', 'of', 'the', 'people.']\n"
     ]
    }
   ],
   "source": [
    "# Generate bigrams and trigrams\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "# # Create gensim dictionary form a single text file\n",
    "# dct = corpora.Dictionary(simple_preprocess(line, deacc=True) for line in open('sample.txt', encoding='utf-8'))\n",
    "\n",
    "\n",
    "# # dataset = [wd for wd in dataset]\n",
    "\n",
    "# # dct = corpora.Dictionary(dataset)\n",
    "# # corpus = [dct.doc2bow(line) for line in dataset]\n",
    "\n",
    "# # Build the bigram models\n",
    "# bigram = gensim.models.phrases.Phrases(dct, min_count=3, threshold=10)\n",
    "\n",
    "# # Construct bigram\n",
    "# print(bigram[dct[0]])\n",
    "\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]\n",
    "\n",
    "documents_1 =[\"The French Revolution was a watershed event in modern European history that began in 1789 and ended in the late 1790s with the ascent of Napoleon Bonaparte. Although it failed to achieve all of its goals and at times degenerated into a chaotic bloodbath, the French Revolution played a critical role in shaping modern nations by showing the world the power inherent in the will of the people.\",\n",
    "              \"During this period, French citizens razed and redesigned their country’s political landscape, uprooting centuries-old institutions such as absolute monarchy and the feudal system. The upheaval was caused by widespread discontent with the French monarchy and the poor economic policies of King Louis XVI, who met his death by guillotine, as did his wife Marie Antoinette. \"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents_1]\n",
    "# print(texts)\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "# print(dictionary)\n",
    "bigram = gensim.models.phrases.Phrases(texts, min_count=3, threshold=10)\n",
    "print(bigram[texts[0]])\n",
    "\n",
    "# Build the trigram models\n",
    "trigram = gensim.models.phrases.Phrases(bigram[texts], threshold=10)\n",
    "\n",
    "# Construct trigram\n",
    "print(trigram[bigram[texts[0]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following can be done using gensim:\n",
    "\n",
    "Generate a Dictionary and a Corpus\n",
    "Create a Dictionary from a list of sentences\n",
    "Create a Dictionary from one or more text files\n",
    "Create a bag of words corpus in gensim\n",
    "Create a bag of words corpus from external text file\n",
    "Save a gensim dictionary and corpus to disk and load them back\n",
    "Create the TFIDF matrix (corpus) in gensim?\n",
    "Use gensim downloader API to load datasets\n",
    "Create bigrams and trigrams using Phraser models\n",
    "Create topic models with LDA\n",
    "Interpret the LDA Topic Model’s output\n",
    "Create a LSI topic model using gensim\n",
    "Train Word2Vec model using gensim\n",
    "Update an existing Word2Vec model with new data\n",
    "Extract word vectors using pre-trained Word2Vec and FastText models\n",
    "Create document vectors using Doc2Vec\n",
    "Compute similarity metrics like cosine similarity and soft cosine similarity\n",
    "Summarize text documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
